---
layout: post
title: "Relating Neural Network Architectures to Physical Measurements"
date: 2019-12-10
---
There exists a breed of classical scientist who mistrusts statistically derived models. It's hard to point to the reasons for any specific step that a neural network takes, or to the physical meaning of some intermediate value in its calculation. Since my work is applying these statistical modeling techniques to physical measurements, I wanted to think through the parallels between neural network architectures (or parts thereof) and existing methods of data analysis from experiments.

The process of conducting measurements to determine some underlying physical property. It is therefore the conversion from some real system into a new space of measured values. This conversion is typically destructive—as a rule there are properties of the system we do not wish to sample. For example, if we wish to learn the density of a piece of metal we would measure its weight and its displacement, but might not record information about its color.

What follows next is the process of analysis. The information we wish to recover about the real world may not be directly measurable. We therefore apply our understanding of physical reality in general to the data we have collected. In the example above we divide the weight of the object by the local acceleration due to gravity in order to calculate its mass. This process may or may not be destructive. In the world of classical mechanics it typically is not; the relationship between dependent quantities is treated as deterministic—the statistical relationships between quantities are small enough to reasonably ignore. In systems which must be treated with quantum mechanics (e.g. measurements of radioactivity) we know that our measurements are samples from an underlying distribution. We may have some *a priori* understanding of our distribution (in the case of radioactive decay we know that the Poisson distribution applies), or we may have to take multiple measurements in order to approximate the true distribution of the data. This also applies to the (more common) processes of predictive machine learning/data science where there is some underlying relationship between data and the quantities we wish to predict. The relationship exists, but we don’t know what form it takes so we infer it from the data.

Simulation could be viewed as the complement of analysis. Instead of transforming (processed/measured) data into the underlying values of interest we start with the information we want and attempt to reproduce the data that would result. The simulation (and representative process) could be either deterministic or probabilistic. In the latter case we would have to run even a perfect simulation a number of times (to reproduce the statistical distribution we would have measured) or run our simulation in such a way that we track the uncertainty from start to finish (increasing the number of parameters to keep track of since we statistical distributions may require more parameters to describe[^1]).

How does this relate to neural networks? Several architectures use statistical inference to approximate the processes above, often in combination with each other. Autoencoders compress measured data into some smaller number of variables, then attempt to use the values of those variables to reproduce the underlying data. This is analogous to **analysis** or possibly **measurement** plus **analysis**, followed by simulation. Variational autoencoders are similar, but they assume that the underlying proceses are statistical and therefore try to learn the properties of the underlying distribution. Generative adversarial networks perform similar steps but in reverse. They perform simulation from random parameters, then evaluate their own performance against real data by attempting to discriminate between the generated (simulated) data and a measurement.

So the network architectures being developed today have close analogues to existing work in physical experiments and data analysis. The tried-and-true tools are still relevant, there are just new methods for approaching the same tasks. This is where I think that data science has the most interesting application to the physical sciences--not for their predictive power, but for the adaptive structures that lead to that predictive power. If a model is performing well on a representative, cross-validated data set, then it's worth examining the relationships between the data that the model is performing and mapping those to some physical meaning. This can push science forward. It's not just black box stuff.

[^1]: This depends on what you assume/know the distribution to be. The Gaussian (also known as the normal distribution or “bell curve”) distribution is defined by two parameters: the mean and the standard deviation. The Poisson distribution is defined by only one parameter (the mean).